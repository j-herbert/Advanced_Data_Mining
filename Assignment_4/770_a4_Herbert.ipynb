{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFS 770 - Advanced Data Mining Application\n",
    "## Assignment 4\n",
    "### John Herbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1: Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('amazon_review_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>helpful</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000GAYQL8</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>GREAT WATCH AND GREAT LOOK. BIG FACE AND 4 DIF...</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000IBNPDA</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>Bought this as a Christmas gift, my boyfriend ...</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000J2HA16</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>I love this watch! Its sporty, without looking...</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000BDIQPM</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>Works great,looks nice,dont have to worry abou...</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000GZTH9E</td>\n",
       "      <td>0/3</td>\n",
       "      <td>4</td>\n",
       "      <td>I need to change the watch wrist and I havent ...</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid helpful  score  \\\n",
       "0  B000GAYQL8     0/0      5   \n",
       "1  B000IBNPDA     0/0      5   \n",
       "2  B000J2HA16     0/0      5   \n",
       "3  B000BDIQPM     0/0      5   \n",
       "4  B000GZTH9E     0/3      4   \n",
       "\n",
       "                                                text category  \n",
       "0  GREAT WATCH AND GREAT LOOK. BIG FACE AND 4 DIF...    watch  \n",
       "1  Bought this as a Christmas gift, my boyfriend ...    watch  \n",
       "2  I love this watch! Its sporty, without looking...    watch  \n",
       "3  Works great,looks nice,dont have to worry abou...    watch  \n",
       "4  I need to change the watch wrist and I havent ...    watch  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     595\n",
      "2     259\n",
      "3     303\n",
      "4     773\n",
      "5    2070\n",
      "Name: score, dtype: int64\n",
      "automotive     1000\n",
      "electronics    1000\n",
      "software       1000\n",
      "watch          1000\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"score\"].value_counts().sort_index())\n",
    "print(data[\"category\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  watch     use     one    work    time    like product   great     get   would \n",
      "   2553    2476    1795    1605    1420    1375    1336    1318    1309    1217 \n"
     ]
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def before_token(documents):\n",
    "    # conver words to lower case\n",
    "    lower = map(str.lower, documents)\n",
    "    # remove puntuations\n",
    "    punctuationless = list(map(lambda x: \" \".join(re.findall('\\\\b\\\\w\\\\w+\\\\b',x)), lower))\n",
    "    # remove numbers\n",
    "    return list(map(lambda x:re.sub('\\\\b[0-9]+\\\\b', '', x), punctuationless))\n",
    "\n",
    "# initialize a stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# define a function that preprocess a single document and returns a list of tokens\n",
    "def preprocess(doc):\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token not in stopwords:\n",
    "            tokens.append(stemmer.stem(token))\n",
    "    return tokens\n",
    "            \n",
    "# preprocess all documents\n",
    "processed = list(map(preprocess, before_token(data['text'])))\n",
    "\n",
    "# calculate the token frequency\n",
    "# the FreqDist function takes in a list of tokens and return a dict containg unique tokens and frequency\n",
    "fdist = nltk.FreqDist([token for doc in processed for token in doc])\n",
    "fdist.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3: Top 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that would not work will in clustering and classification if I was attempting to determine what each cluster's topic was are\n",
    "* **use**\n",
    "* **one**\n",
    "* **like**\n",
    "* **great**\n",
    "* **get**\n",
    "* **would**\n",
    "* **Product**\n",
    "\n",
    "These words are general, and may not be specific to a cluster depending on the topics we are predicting. In this case, we know what each topic is and these are not specific to any of the 4 categories. They more describe the use or sentiment of the products. Since each topic is about reviews of products from Amazon, **product** is too general and describes items within each of the catagories.\n",
    "\n",
    "However, **watch** and **time** are good descriptors of the **watch** category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4: Reconstruct the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features = 10833\n"
     ]
    }
   ],
   "source": [
    "processed_doc = list(map(\" \".join, processed))\n",
    "# normalization is needed for clustering\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, stop_words='english') # Default norm is 'l2'\n",
    "X = vectorizer.fit_transform(processed_doc)\n",
    "\n",
    "print('The number of features = %d' % X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5: K-means Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " use\n",
      " product\n",
      " work\n",
      " great\n",
      " good\n",
      " instal\n",
      " program\n",
      " like\n",
      " softwar\n",
      " time\n",
      "Cluster 1:\n",
      " batteri\n",
      " charg\n",
      " charger\n",
      " power\n",
      " adapt\n",
      " appl\n",
      " camera\n",
      " canon\n",
      " work\n",
      " origin\n",
      "Cluster 2:\n",
      " watch\n",
      " look\n",
      " band\n",
      " time\n",
      " great\n",
      " wear\n",
      " love\n",
      " like\n",
      " nice\n",
      " price\n",
      "Cluster 3:\n",
      " bed\n",
      " air\n",
      " inflat\n",
      " comfort\n",
      " pump\n",
      " sleep\n",
      " mattress\n",
      " deflat\n",
      " airb\n",
      " easi\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=4, max_iter=100, random_state=4)\n",
    "km.fit(X)\n",
    "km.transform(X)\n",
    "\n",
    "# examine the representative words for each cluster\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(4):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T6: Cluster Examination and Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the clusters do a good job at explaining each of the 4 categories of the amazon reviews in the data set.\n",
    "* **Cluster 0**: Appears to be focused on the 'software' category as there are features like 'install', 'program', and 'softwar'. It is interesting that that softwar is missing the 'e' at end, however this may be due to the stemmer bringing it to its base word as shown above. However, 'time' appears in this cluster which may be related to the 'watch' category.\n",
    "* **Cluster 1**: Appears to be focused on the 'electronic' category. There are features such as 'batteri', 'char', 'charger', 'power', 'canon', and 'adapt' that suggest this is the category. However, if 'appl' was not in the top 10, I might mistake this for a camera category (if I did not know the categories ahead of time). Also, this category could partially be related to the automotive category as it takes about adapters and batteries.\n",
    "* **Cluster 2**: Appears to be focused on the 'watch' category. There are features such as 'watch', 'band', 'time',  and 'wear'. The other features in this cluster appear to be forcused on the sentiment of the products, not the category.\n",
    "* **Cluster 3**: This cluster does an overall poor job describing the category. It has features such as 'air', 'bed', 'inflate', 'sleep', 'mattress' which would imply that this category is on inflattable mattresses and not 'autmotive' the only remaining category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems of software and apple\n",
      "softwar\n",
      "appl\n"
     ]
    }
   ],
   "source": [
    "print('Stems of software and apple')\n",
    "print(stemmer.stem(\"software\"))\n",
    "print(stemmer.stem(\"appl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T7: Gensim LDA Model Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"use\" + 0.006*\"work\" + 0.005*\"product\" + 0.005*\"great\" + 0.004*\"good\" + 0.004*\"time\" + 0.004*\"batteri\" + 0.004*\"like\" + 0.004*\"easi\" + 0.003*\"instal\"'),\n",
       " (1,\n",
       "  '0.030*\"watch\" + 0.008*\"band\" + 0.006*\"look\" + 0.006*\"wear\" + 0.005*\"love\" + 0.004*\"wrist\" + 0.004*\"face\" + 0.004*\"beauti\" + 0.003*\"nice\" + 0.003*\"great\"'),\n",
       " (2,\n",
       "  '0.002*\"cartridg\" + 0.001*\"cannon\" + 0.001*\"printer\" + 0.001*\"dud\" + 0.001*\"matress\" + 0.001*\"pic\" + 0.001*\"deskjet\" + 0.001*\"bilstein\" + 0.001*\"sd950i\" + 0.001*\"bronco\"'),\n",
       " (3,\n",
       "  '0.003*\"mop\" + 0.001*\"nissan\" + 0.001*\"la\" + 0.001*\"el\" + 0.001*\"en\" + 0.001*\"es\" + 0.001*\"gasket\" + 0.001*\"vw\" + 0.001*\"producto\" + 0.001*\"radioand\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the vectorized data to a gensim corpus object\n",
    "word2id = dict((k, v) for k, v in vectorizer.vocabulary_.items())\n",
    "id2word = dict((v, k) for k,v in vectorizer.vocabulary_.items())\n",
    "d=corpora.Dictionary()\n",
    "d.id2token = id2word\n",
    "d.token2id = word2id\n",
    "corpus = matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# build the lda model after we find the optimal number of topics\n",
    "lda = LdaModel(corpus, num_topics=4,id2word=id2word, random_state=42, passes=30)\n",
    "\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T8: LDA Examination and Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall these features do a poor job at explaining the categories:\n",
    "* **Category 0**: This is unclear the category of this topic as it has *batteri*, and *install* which could be either software, automotive, or electronics. It does have *time*, but also *install* so it is unclear if watch would be a category\n",
    "* **Category 1**: This is fairly clear the topic is watch as the features select are *watch*, *band*, *wrist*, and *face* with only neutral tterms as the top features.\n",
    "* **Category 2**: This category is ambiguous as it has *cannon*, *printer*, and *deskject* which may suggest it is electronic, however it also has *bronco* which might suggest automotive.\n",
    "* **Category 3**: This category appears to be about automotive asd it has *gasket*, *vw*, *nissan* and *radio* which are components or a make of a vehicle with no ambiguous terms that are confusing.\n",
    "\n",
    "Overall I believe the clustering does a better job at categorizing the data. It generally gives features that are easier to decipher what the category is for 3 of 4 topics. However, the LDA does a good job at predicting the automotive category. The LDA does however tend to have more useless topic features in the list than the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T9: 5-fold CV SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlisting processed data to insert as column in data frame \n",
    "# Using processed data from earlier task to run through the SVM model\n",
    "processed2 = [' '.join(c for c in lst) for lst in processed]\n",
    "data['proc_text'] = processed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.33      0.44       119\n",
      "           2       0.27      0.06      0.10        51\n",
      "           3       0.04      0.02      0.02        61\n",
      "           4       0.27      0.17      0.21       155\n",
      "           5       0.62      0.91      0.74       414\n",
      "\n",
      "    accuracy                           0.56       800\n",
      "   macro avg       0.37      0.30      0.30       800\n",
      "weighted avg       0.49      0.56      0.49       800\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.59      0.50       119\n",
      "           2       0.12      0.06      0.08        52\n",
      "           3       0.18      0.15      0.17        60\n",
      "           4       0.27      0.23      0.25       155\n",
      "           5       0.67      0.70      0.68       414\n",
      "\n",
      "    accuracy                           0.51       800\n",
      "   macro avg       0.34      0.35      0.34       800\n",
      "weighted avg       0.49      0.51      0.50       800\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.70      0.60       119\n",
      "           2       0.23      0.13      0.17        52\n",
      "           3       0.18      0.10      0.13        60\n",
      "           4       0.31      0.32      0.31       155\n",
      "           5       0.71      0.72      0.72       414\n",
      "\n",
      "    accuracy                           0.56       800\n",
      "   macro avg       0.39      0.39      0.39       800\n",
      "weighted avg       0.54      0.56      0.54       800\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.59      0.61       119\n",
      "           2       0.35      0.13      0.19        52\n",
      "           3       0.24      0.08      0.12        61\n",
      "           4       0.38      0.25      0.30       154\n",
      "           5       0.65      0.86      0.74       414\n",
      "\n",
      "    accuracy                           0.59       800\n",
      "   macro avg       0.45      0.38      0.39       800\n",
      "weighted avg       0.54      0.59      0.55       800\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.41      0.50       119\n",
      "           2       0.33      0.08      0.12        52\n",
      "           3       0.24      0.10      0.14        61\n",
      "           4       0.37      0.14      0.20       154\n",
      "           5       0.60      0.92      0.73       414\n",
      "\n",
      "    accuracy                           0.57       800\n",
      "   macro avg       0.44      0.33      0.34       800\n",
      "weighted avg       0.52      0.57      0.51       800\n",
      "\n",
      "Average F1: 0.52\n",
      "The number of features = 5067\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(data[\"proc_text\"], data[\"score\"]):\n",
    "#for train_index, test_index in skf:\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = data[\"proc_text\"].iloc[train_index], data[\"proc_text\"].iloc[test_index]\n",
    "    train_y, test_y = data[\"score\"].iloc[train_index], data[\"score\"].iloc[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, stop_words='english',min_df=2) \n",
    "    # min_df removes terms that appear less than a given threshold, therefore setting it to 2\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold) # Stochastic gradient desent SVM\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y, average='weighted'))\n",
    "\n",
    "   \n",
    "print(\"Average F1: %.2f\" % np.mean(f1))\n",
    "print('The number of features = %d' % X_test.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T10: Create New Variable: satisfaction & 5-fold CV SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.23      0.35       231\n",
      "           1       0.76      0.98      0.86       569\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.80      0.60      0.61       800\n",
      "weighted avg       0.78      0.76      0.71       800\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.73      0.67       231\n",
      "           1       0.88      0.82      0.85       569\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.75      0.77      0.76       800\n",
      "weighted avg       0.81      0.79      0.80       800\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.67       231\n",
      "           1       0.88      0.84      0.86       569\n",
      "\n",
      "    accuracy                           0.80       800\n",
      "   macro avg       0.76      0.77      0.77       800\n",
      "weighted avg       0.81      0.80      0.81       800\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65       232\n",
      "           1       0.84      0.93      0.88       568\n",
      "\n",
      "    accuracy                           0.83       800\n",
      "   macro avg       0.81      0.75      0.77       800\n",
      "weighted avg       0.82      0.83      0.82       800\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.44      0.57       232\n",
      "           1       0.81      0.96      0.88       568\n",
      "\n",
      "    accuracy                           0.81       800\n",
      "   macro avg       0.81      0.70      0.72       800\n",
      "weighted avg       0.81      0.81      0.79       800\n",
      "\n",
      "Average F1: 0.78\n",
      "The number of features = 5070\n"
     ]
    }
   ],
   "source": [
    "# Creating a new variable 'satisfaction' where scores of 4 and 5 = 1, all else = 0\n",
    "data['satisfaction'] = data['score'].apply(lambda x: 1 if x == 4 or x == 5 else 0 )\n",
    "\n",
    "# 5-fold cross validation\n",
    "fold = 0\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(data[\"proc_text\"], data[\"satisfaction\"]):\n",
    "#for train_index, test_index in skf:\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = data[\"proc_text\"].iloc[train_index], data[\"proc_text\"].iloc[test_index]\n",
    "    train_y, test_y = data[\"satisfaction\"].iloc[train_index], data[\"satisfaction\"].iloc[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, stop_words='english',min_df=2) \n",
    "    # min_df removes terms that appear less than a given threshold, therefore setting it to 2\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold) # Stochastic gradient desent SVM\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y, average='weighted'))\n",
    "\n",
    "   \n",
    "print(\"Average F1: %.2f\" % np.mean(f1))\n",
    "print('The number of features = %d' % X_test.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T11: Bing Liu's Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lexicon\n",
    "lexicon = dict()\n",
    "\n",
    "# read postive words\n",
    "with open(\"negative-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = -1\n",
    "\n",
    "# read negative words\n",
    "with open(\"positive-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.25      0.36       231\n",
      "           1       0.76      0.94      0.84       569\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.70      0.60      0.60       800\n",
      "weighted avg       0.72      0.74      0.70       800\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.51      0.59       231\n",
      "           1       0.82      0.91      0.86       569\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.75      0.71      0.72       800\n",
      "weighted avg       0.78      0.79      0.78       800\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.47      0.57       231\n",
      "           1       0.81      0.92      0.86       569\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.76      0.70      0.71       800\n",
      "weighted avg       0.78      0.79      0.78       800\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.46      0.55       232\n",
      "           1       0.81      0.92      0.86       568\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.75      0.69      0.71       800\n",
      "weighted avg       0.78      0.79      0.77       800\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.38      0.44       232\n",
      "           1       0.77      0.87      0.82       568\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.66      0.62      0.63       800\n",
      "weighted avg       0.71      0.73      0.71       800\n",
      "\n",
      "Average F1: 0.75\n",
      "The number of features = 6786\n"
     ]
    }
   ],
   "source": [
    "# Setting the vocab arugment to the Bing Liu's Lexicon\n",
    "vocab = lexicon.keys()\n",
    "# 5-fold cross validation\n",
    "fold = 0\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(data[\"proc_text\"], data[\"satisfaction\"]):\n",
    "#for train_index, test_index in skf:\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = data[\"proc_text\"].iloc[train_index], data[\"proc_text\"].iloc[test_index]\n",
    "    train_y, test_y = data[\"satisfaction\"].iloc[train_index], data[\"satisfaction\"].iloc[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, stop_words='english',min_df=2,vocabulary=vocab) \n",
    "    # min_df removes terms that appear less than a given threshold, therefore setting it to 2\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold) # Stochastic gradient desent SVM\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y, average='weighted'))\n",
    "\n",
    "   \n",
    "print(\"Average F1: %.2f\" % np.mean(f1))\n",
    "print('The number of features = %d' % X_test.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T12: Comparing T10 & T11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score across all 5 of the cross validation folds for T10 (using a SGD classifier across all words in the corpus excluding stop words) is 0.78. \n",
    "\n",
    "The average F1 score across all 5 of the cross validation folds for T11 (using a SGD classifier across just the words in the lexicon dictionary) is 0.75.\n",
    "\n",
    "Generally, it is better practice to use all the words in the corpus instead of just the words in the a specific dictionary. This is most likely because there are words that have meaning on whether a customer was satisfied or not that are taken out when using the lexicon dictionary. Therefore, the SGD clasifier does a better job when it has more features to train with and make predictions on the clasification when not using the lexicon as a dictionary in the vectorizor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T13: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.99642752e-03 3.34949529e-03 2.53746962e-03 ... 3.36972499e-38\n",
      " 1.10980541e-38 4.60171578e-39]\n"
     ]
    }
   ],
   "source": [
    "# Vecotrizing the data using the same parameters from T9\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, stop_words='english',min_df=2) \n",
    "X = vectorizer.fit_transform(data[\"proc_text\"]).todense()\n",
    "#X_std = StandardScaler().fit_transform(X) # you need to do standardization, since pca is sensitive \n",
    "# to the relative scaling of the original variables\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Running PCA variable selection that explain 90%\n",
    "pca = PCA(svd_solver='randomized',whiten=True).fit(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T14: PCA Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a dimension reduction method that is generally used for variable reduction in a model. It does this by creating new variables called principal components created by a reconstruction of the variables into linear combinations of the initial variables. Eachof thse principal components are essentially a dimension in the model. So if you have 10 predictors, there are 10 dimensions to the model, therefore 10 principals components. PCA attempts to put maximum information in the first component, then maximum remaining information in the second and so on. Therefore, it is easy to identify which components can be removed without losing information on the variances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T15: PCA Variable Reduction and SGD Classifier with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components selected by PCA = 2017\n"
     ]
    }
   ],
   "source": [
    "sumofvariance=0.0\n",
    "n_components = 0\n",
    "for item in pca.explained_variance_ratio_:\n",
    "    sumofvariance += item\n",
    "    n_components+=1\n",
    "    if sumofvariance>=0.9:\n",
    "        break\n",
    "print('The number of components selected by PCA = %d' % n_components) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.49      0.37       231\n",
      "           1       0.72      0.52      0.61       569\n",
      "\n",
      "    accuracy                           0.51       800\n",
      "   macro avg       0.51      0.51      0.49       800\n",
      "weighted avg       0.59      0.51      0.54       800\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.23      0.27       231\n",
      "           1       0.72      0.81      0.76       569\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.52      0.52      0.52       800\n",
      "weighted avg       0.61      0.64      0.62       800\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.39      0.34       231\n",
      "           1       0.72      0.63      0.67       569\n",
      "\n",
      "    accuracy                           0.56       800\n",
      "   macro avg       0.51      0.51      0.51       800\n",
      "weighted avg       0.60      0.56      0.58       800\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.22      0.24       232\n",
      "           1       0.70      0.75      0.73       568\n",
      "\n",
      "    accuracy                           0.60       800\n",
      "   macro avg       0.48      0.48      0.48       800\n",
      "weighted avg       0.57      0.60      0.58       800\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.24      0.28       232\n",
      "           1       0.73      0.83      0.77       568\n",
      "\n",
      "    accuracy                           0.66       800\n",
      "   macro avg       0.54      0.53      0.53       800\n",
      "weighted avg       0.62      0.66      0.63       800\n",
      "\n",
      "Average F1: 0.59\n"
     ]
    }
   ],
   "source": [
    "X_train_pca = pca.transform(X)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(X_train_pca, data[\"satisfaction\"]):\n",
    "#for train_index, test_index in skf:\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = X_train_pca[train_index], X_train_pca[test_index]\n",
    "    train_y, test_y = data[\"satisfaction\"].iloc[train_index], data[\"satisfaction\"].iloc[test_index]\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold) # Stochastic gradient desent SVM\n",
    "    clf.fit(train_x, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(test_x)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    f1.append(metrics.f1_score(test_y, pred_y, average='weighted'))\n",
    "\n",
    "print(\"Average F1: %.2f\" % np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T16: Explanation of PCA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score from the model using the PCA calculated variables was 0.59. This was worse than the model in T10, which was 0.78. This is because the SGD Classifier uses all the features, excluding stop words and features that appear in only one document. This is because PCA generally works better when the features are highly correlated and variable reduction is needed to reduce the noise and focus on the features bringing new information. It appears that there is not much multicolinearity between the categories as each is fairly different and unique. However, if 2 of the categories were, for example, 'personal computers' and 'laptops', we may see better scores with the PCA model as there is a lot of overlap between these 2 topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
